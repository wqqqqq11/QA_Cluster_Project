# main.py
import os
import sys
import time
import numpy as np
import pandas as pd

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.utils import (
    ensure_dir,
    load_qa_excel,
    extract_questions,
    build_text_embeddings,
    save_numpy_vectors,
    save_feature_preview_excel,
    quick_similarity_sanity_check,
    choose_best_k_by_silhouette,
    run_minibatch_kmeans,
    save_dataset_cluster_excel,
    save_cluster_summary_excel,
    save_cluster_answer_review_excel,   # NEW (Step3)
    get_embedding_model_name,
)
from src.agent import add_cluster_names_to_file

PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_DIR = os.path.join(PROJECT_ROOT, "data")
OUTPUT_DIR = os.path.join(PROJECT_ROOT, "output")
VECTORIZED_DATA_DIR = os.path.join(PROJECT_ROOT, "vectorized_data")

# ç»Ÿè®¡ä¿¡æ¯å­˜å‚¨
stats_info = {
    'stage_times': {},
    'data_scales': {}
}


def stage1_feature_extraction_tianmao():
    """å¤„ç†tianmaoæ•°æ®é›†çš„ç‰¹å¾æå–"""
    start_time = time.time()
    
    ensure_dir(OUTPUT_DIR)
    ensure_dir(VECTORIZED_DATA_DIR)

    dataset_path = os.path.join(DATA_DIR, "meaningful_answer_tianmao.xlsx")
    # tianmaoæ•°æ®é›†åˆ—åå·²ç»æ˜¯æ ‡å‡†çš„"é—®é¢˜"å’Œ"å›ç­”"
    df = load_qa_excel(dataset_path, column_mapping={"é—®é¢˜": "é—®é¢˜", "å›ç­”": "å›ç­”"})

    questions = extract_questions(df, question_col="é—®é¢˜")

    vectors, model_info = build_text_embeddings(
        texts=questions,
        model_name=get_embedding_model_name(),
        batch_size=64,
        normalize=True,
        device="cpu"
    )

    npy_path = os.path.join(VECTORIZED_DATA_DIR, "tianmao_question_vectors.npy")
    save_numpy_vectors(vectors, npy_path)

    preview_path = os.path.join(OUTPUT_DIR, "tianmao_feature_preview.xlsx")
    save_feature_preview_excel(
        df=df,
        questions=questions,
        vectors=vectors,
        out_path=preview_path,
        model_info=model_info,
        question_col="é—®é¢˜",
    )

    quick_similarity_sanity_check(questions, vectors, topk=5)

    # è®°å½•ç»Ÿè®¡ä¿¡æ¯
    stats_info['stage_times']['tianmao_feature_extraction'] = time.time() - start_time
    stats_info['data_scales']['tianmao_dataset'] = len(questions)

    print("\n[OK] Tianmao feature extraction done.")
    print(f" - Saved vectors: {npy_path} (in vectorized_data/)")
    print(f" - Saved preview: {preview_path}")

    return df, questions, vectors


def stage1_feature_extraction_overseas():
    """å¤„ç†overseasæ•°æ®é›†çš„ç‰¹å¾æå–"""
    start_time = time.time()
    
    ensure_dir(OUTPUT_DIR)
    ensure_dir(VECTORIZED_DATA_DIR)

    dataset_path = os.path.join(DATA_DIR, "meaningful_answer_overseas.xlsx")
    # overseasæ•°æ®é›†éœ€è¦åˆ—åæ˜ å°„
    df = load_qa_excel(dataset_path, column_mapping={"é—®é¢˜": "å®¢æˆ·é—®é¢˜", "å›ç­”": "å®¢æœå›å¤"})

    questions = extract_questions(df, question_col="é—®é¢˜")

    vectors, model_info = build_text_embeddings(
        texts=questions,
        model_name=get_embedding_model_name(),
        batch_size=64,
        normalize=True,
        device="cpu"
    )

    npy_path = os.path.join(VECTORIZED_DATA_DIR, "overseas_question_vectors.npy")
    save_numpy_vectors(vectors, npy_path)

    preview_path = os.path.join(OUTPUT_DIR, "overseas_feature_preview.xlsx")
    save_feature_preview_excel(
        df=df,
        questions=questions,
        vectors=vectors,
        out_path=preview_path,
        model_info=model_info,
        question_col="é—®é¢˜",
    )

    quick_similarity_sanity_check(questions, vectors, topk=5)

    # è®°å½•ç»Ÿè®¡ä¿¡æ¯
    stats_info['stage_times']['overseas_feature_extraction'] = time.time() - start_time
    stats_info['data_scales']['overseas_dataset'] = len(questions)

    print("\n[OK] Overseas feature extraction done.")
    print(f" - Saved vectors: {npy_path} (in vectorized_data/)")
    print(f" - Saved preview: {preview_path}")

    return df, questions, vectors


def stage2_clustering_dataset1(
    k_min: int = 5,
    k_max: int = 30,
    choose_k: bool = True,
    fixed_k: int = 12,
):
    """
    Task1-Step(2): clustering execution (MiniBatchKMeans)
    Outputs:
      output/dataset1_cluster.xlsx
      output/dataset1_cluster_summary.xlsx
    """
    ensure_dir(OUTPUT_DIR)

    dataset_path = os.path.join(DATA_DIR, "dataset1.xlsx")
    df = load_qa_excel(dataset_path)
    questions = extract_questions(df, question_col="é—®é¢˜")

    vectors, _ = build_text_embeddings(
        texts=questions,
        model_name=get_embedding_model_name(),
        batch_size=64,
        normalize=True,
        device="cpu"
    )

    # 1) choose best k
    if choose_k:
        best_k, eval_df = choose_best_k_by_silhouette(
            vectors=vectors,
            k_min=k_min,
            k_max=k_max,
            sample_size=4000,
            random_state=42
        )
        print("\n[K-Selection] silhouette scores:")
        print(eval_df.to_string(index=False))
        print(f"\n[K-Selection] best_k={best_k}")
        k = best_k
    else:
        k = fixed_k
        print(f"\n[K-Selection] use fixed_k={k}")

    # 2) run clustering
    labels, _model = run_minibatch_kmeans(
        vectors=vectors,
        n_clusters=k,
        random_state=42,
        batch_size=1024,
        max_iter=200
    )

    # 3) save per-row cluster result
    out_cluster_path = os.path.join(OUTPUT_DIR, "dataset1_cluster.xlsx")
    save_dataset_cluster_excel(
        df=df,
        questions=questions,
        labels=labels,
        out_path=out_cluster_path,
        question_col="é—®é¢˜",
        answer_col="å›ç­”"
    )

    # 4) save cluster summary for manual review (questions-focused)
    out_summary_path = os.path.join(OUTPUT_DIR, "dataset1_cluster_summary.xlsx")
    save_cluster_summary_excel(
        df=df,
        questions=questions,
        vectors=vectors,
        labels=labels,
        out_path=out_summary_path,
        top_examples=99999  # æ˜¾ç¤ºæ‰€æœ‰å±äºè¯¥èšç±»çš„é—®é¢˜
    )

    print("\n[OK] Stage2 clustering done.")
    print(f" - Saved clustered dataset: {out_cluster_path}")
    print(f" - Saved cluster summary:   {out_summary_path}")

    return out_cluster_path


def stage1_merge_datasets():
    """åˆå¹¶æ‰€æœ‰æ•°æ®é›†è¿›è¡Œç»Ÿä¸€èšç±»åˆ†æ"""
    start_time = time.time()
    
    ensure_dir(OUTPUT_DIR)
    ensure_dir(VECTORIZED_DATA_DIR)
    
    print("\n[Merge] å¼€å§‹åˆå¹¶å¤šæ•°æ®é›†...")
    
    # å¤„ç†tianmaoæ•°æ®é›†
    print("[Merge] å¤„ç†tianmaoæ•°æ®é›†...")
    df_tianmao, questions_tianmao, vectors_tianmao = stage1_feature_extraction_tianmao()
    df_tianmao['source_dataset'] = 'tianmao'
    
    # å¤„ç†overseasæ•°æ®é›†
    print("[Merge] å¤„ç†overseasæ•°æ®é›†...")
    df_overseas, questions_overseas, vectors_overseas = stage1_feature_extraction_overseas()
    df_overseas['source_dataset'] = 'overseas'
    
    # åˆå¹¶æ•°æ®
    print("[Merge] åˆå¹¶æ•°æ®...")
    all_dfs = [df_tianmao, df_overseas]
    all_questions = questions_tianmao + questions_overseas
    all_vectors = np.vstack([vectors_tianmao, vectors_overseas])
    
    # åˆå¹¶DataFrameï¼Œä¿æŒç´¢å¼•è¿ç»­
    merged_df = pd.concat(all_dfs, ignore_index=True)
    
    print(f"[Merge] åˆå¹¶å®Œæˆ:")
    print(f" - Tianmao: {len(questions_tianmao)} ä¸ªé—®é¢˜")
    print(f" - Overseas: {len(questions_overseas)} ä¸ªé—®é¢˜")
    print(f" - æ€»è®¡: {len(all_questions)} ä¸ªé—®é¢˜")
    
    # ä¿å­˜åˆå¹¶åçš„å‘é‡
    merged_vectors_path = os.path.join(VECTORIZED_DATA_DIR, "merged_question_vectors.npy")
    save_numpy_vectors(all_vectors, merged_vectors_path)
    
    # ä¿å­˜åˆå¹¶åçš„ç‰¹å¾é¢„è§ˆ
    merged_preview_path = os.path.join(OUTPUT_DIR, "merged_feature_preview.xlsx")
    save_feature_preview_excel(
        df=merged_df,
        questions=all_questions,
        vectors=all_vectors,
        out_path=merged_preview_path,
        model_info={'backend': 'sentence-transformers', 'model_name': get_embedding_model_name(), 'device': 'cpu', 'normalize': True, 'dim': 384, 'count': len(all_questions)},
        question_col="é—®é¢˜",
    )
    
    quick_similarity_sanity_check(all_questions, all_vectors, topk=5)
    
    # è®°å½•ç»Ÿè®¡ä¿¡æ¯
    stats_info['stage_times']['merge_datasets'] = time.time() - start_time
    stats_info['data_scales']['merged_dataset'] = len(all_questions)
    
    print(f"\n[OK] æ•°æ®é›†åˆå¹¶å®Œæˆ.")
    print(f" - ä¿å­˜åˆå¹¶å‘é‡: {merged_vectors_path} (in vectorized_data/)")
    print(f" - ä¿å­˜åˆå¹¶é¢„è§ˆ: {merged_preview_path}")
    
    return merged_df, all_questions, all_vectors


def stage2_clustering_merged(
    k_min: int = 5,
    k_max: int = 50,
    choose_k: bool = True,
    fixed_k: int = 30,
):
    """
    å¯¹åˆå¹¶æ•°æ®é›†è¿›è¡Œèšç±»åˆ†æ
    """
    start_time = time.time()
    
    ensure_dir(OUTPUT_DIR)
    ensure_dir(VECTORIZED_DATA_DIR)

    # åŠ è½½åˆå¹¶åçš„æ•°æ®
    merged_vectors_path = os.path.join(VECTORIZED_DATA_DIR, "merged_question_vectors.npy")
    if not os.path.exists(merged_vectors_path):
        print("[Error] åˆå¹¶å‘é‡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œstage1_merge_datasets()")
        return None

    vectors = np.load(merged_vectors_path)
    
    # é‡æ–°æ„å»ºåˆå¹¶çš„DataFrameå’Œé—®é¢˜åˆ—è¡¨
    df_tianmao, questions_tianmao, _ = stage1_feature_extraction_tianmao()
    df_overseas, questions_overseas, _ = stage1_feature_extraction_overseas()
    
    df_tianmao['source_dataset'] = 'tianmao'
    df_overseas['source_dataset'] = 'overseas'
    
    merged_df = pd.concat([df_tianmao, df_overseas], ignore_index=True)
    all_questions = questions_tianmao + questions_overseas

    # 1) choose best k
    if choose_k:
        best_k, eval_df = choose_best_k_by_silhouette(
            vectors=vectors,
            k_min=k_min,
            k_max=k_max,
            sample_size=4000,
            random_state=42
        )
        print("\n[K-Selection] silhouette scores:")
        print(eval_df.to_string(index=False))
        print(f"\n[K-Selection] best_k={best_k}")
        k = best_k
    else:
        k = fixed_k
        print(f"\n[K-Selection] use fixed_k={k}")

    # 2) run clustering
    labels, _model = run_minibatch_kmeans(
        vectors=vectors,
        n_clusters=k,
        random_state=42,
        batch_size=1024,
        max_iter=200
    )

    # 3) save per-row cluster result
    out_cluster_path = os.path.join(OUTPUT_DIR, "merged_cluster.xlsx")
    save_dataset_cluster_excel(
        df=merged_df,
        questions=all_questions,
        labels=labels,
        out_path=out_cluster_path,
        question_col="é—®é¢˜",
        answer_col="å›ç­”"
    )

    # 4) save cluster summary for manual review
    out_summary_path = os.path.join(OUTPUT_DIR, "merged_cluster_summary.xlsx")
    save_cluster_summary_excel(
        df=merged_df,
        questions=all_questions,
        vectors=vectors,
        labels=labels,
        out_path=out_summary_path,
        top_examples=99999  # æ˜¾ç¤ºæ‰€æœ‰å±äºè¯¥èšç±»çš„é—®é¢˜
    )

    # è®°å½•ç»Ÿè®¡ä¿¡æ¯
    stats_info['stage_times']['clustering_merged'] = time.time() - start_time
    stats_info['data_scales']['clustering_k'] = k

    print("\n[OK] åˆå¹¶æ•°æ®é›†èšç±»å®Œæˆ.")
    print(f" - ä¿å­˜èšç±»æ•°æ®é›†: {out_cluster_path}")
    print(f" - ä¿å­˜èšç±»æ‘˜è¦: {out_summary_path}")

    return out_cluster_path


def stage3_answer_grouping_merged():
    """
    å¯¹åˆå¹¶æ•°æ®é›†çš„èšç±»ç»“æœè¿›è¡Œç­”æ¡ˆåˆ†ç»„
    """
    start_time = time.time()
    
    ensure_dir(OUTPUT_DIR)

    clustered_path = os.path.join(OUTPUT_DIR, "merged_cluster.xlsx")
    if not os.path.exists(clustered_path):
        raise FileNotFoundError(
            "merged_cluster.xlsx not found. Please run stage2_clustering_merged() first."
        )

    df_clustered = pd.read_excel(clustered_path, engine="openpyxl")

    # Ensure we have cleaned question column
    if "é—®é¢˜_clean" not in df_clustered.columns:
        if "é—®é¢˜" in df_clustered.columns:
            df_clustered["é—®é¢˜_clean"] = df_clustered["é—®é¢˜"].astype(str)
        else:
            raise ValueError("No question column found in merged_cluster.xlsx")

    out_path = os.path.join(OUTPUT_DIR, "merged_cluster_answers.xlsx")
    save_cluster_answer_review_excel(
        df_clustered=df_clustered,
        out_path=out_path,
        cluster_col="cluster_id",
        question_col="é—®é¢˜_clean",
        answer_col="å›ç­”",
        top_questions=99999,
        top_answers=99999,
        answer_sep="\n---\n",
    )

    print("\n[OK] åˆå¹¶æ•°æ®é›†ç­”æ¡ˆåˆ†ç»„å®Œæˆ.")
    print(f" - ä¿å­˜ç­”æ¡ˆå®¡æ ¸æ–‡ä»¶: {out_path}")
    
    # æ·»åŠ èšç±»ä¸­æ–‡æ ‡ç­¾
    add_cluster_names_to_file(out_path)
    
    # è®°å½•ç»Ÿè®¡ä¿¡æ¯
    stats_info['stage_times']['answer_grouping_merged'] = time.time() - start_time

    return out_path


def stage3_answer_grouping_dataset1():
    """
    Task1-Step(3): Answer grouping & validation export (dataset1 only)
    Input:
      output/dataset1_cluster.xlsx
    Output:
      output/dataset1_cluster_answers.xlsx
    """
    ensure_dir(OUTPUT_DIR)

    clustered_path = os.path.join(OUTPUT_DIR, "dataset1_cluster.xlsx")
    if not os.path.exists(clustered_path):
        raise FileNotFoundError(
            "dataset1_cluster.xlsx not found. Please run stage2_clustering_dataset1() first."
        )

    df_clustered = pd.read_excel(clustered_path, engine="openpyxl")

    # Ensure we have cleaned question column
    if "é—®é¢˜_clean" not in df_clustered.columns:
        # Backward compatibility: if your file uses other name, try to recover
        if "é—®é¢˜_clean" not in df_clustered.columns and "é—®é¢˜_clean" not in df_clustered.columns:
            # If only åŸå§‹â€œé—®é¢˜â€ exists, treat it as clean for this export
            if "é—®é¢˜" in df_clustered.columns:
                df_clustered["é—®é¢˜_clean"] = df_clustered["é—®é¢˜"].astype(str)
            else:
                raise ValueError("No question column found in dataset1_cluster.xlsx")

    out_path = os.path.join(OUTPUT_DIR, "dataset1_cluster_answers.xlsx")
    save_cluster_answer_review_excel(
        df_clustered=df_clustered,
        out_path=out_path,
        cluster_col="cluster_id",
        question_col="é—®é¢˜_clean",
        answer_col="å›ç­”",
        top_questions=99999,
        top_answers=99999,
        answer_sep="\n---\n",
    )

    print("\n[OK] Stage3 answer grouping done.")
    print(f" - Saved answer review file: {out_path}")
    
    # æ·»åŠ èšç±»ä¸­æ–‡æ ‡ç­¾
    add_cluster_names_to_file(out_path)


def print_final_statistics():
    """æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
    print("\n" + "=" * 80)
    print("ğŸ“Š æ•°æ®å¤„ç†ç»Ÿè®¡ä¿¡æ¯")
    print("=" * 80)
    
    print("\nğŸ“ˆ æ•°æ®è§„æ¨¡ç»Ÿè®¡:")
    print(f"  â€¢ å¤©çŒ«æ•°æ®é›†: {stats_info['data_scales'].get('tianmao_dataset', 0):,} ä¸ªé—®é¢˜")
    print(f"  â€¢ æµ·å¤–æ•°æ®é›†: {stats_info['data_scales'].get('overseas_dataset', 0):,} ä¸ªé—®é¢˜")
    print(f"  â€¢ åˆå¹¶æ•°æ®é›†: {stats_info['data_scales'].get('merged_dataset', 0):,} ä¸ªé—®é¢˜")
    print(f"  â€¢ èšç±»ç°‡æ•°é‡: {stats_info['data_scales'].get('clustering_k', 0)} ä¸ª")
    
    print("\nâ±ï¸  å„é˜¶æ®µè€—æ—¶ç»Ÿè®¡:")
    stage_names = {
        'tianmao_feature_extraction': 'ğŸ“Š å¤©çŒ«æ•°æ®ç‰¹å¾æå–',
        'overseas_feature_extraction': 'ğŸ“Š æµ·å¤–æ•°æ®ç‰¹å¾æå–',
        'merge_datasets': 'ğŸ“Š æ•°æ®é›†åˆå¹¶',
        'clustering_merged': 'ğŸ¯ åˆå¹¶æ•°æ®é›†èšç±»',
        'answer_grouping_merged': 'ğŸ“ ç­”æ¡ˆåˆ†ç»„å’Œæ ‡ç­¾ç”Ÿæˆ'
    }
    
    total_time = 0
    for stage_key, stage_name in stage_names.items():
        duration = stats_info['stage_times'].get(stage_key, 0)
        total_time += duration
        minutes, seconds = divmod(duration, 60)
        print(f"  â€¢ {stage_name}: {minutes:.0f}åˆ†{seconds:.1f}ç§’")
    
    total_minutes, total_seconds = divmod(total_time, 60)
    print(f"\nğŸš€ æ€»è®¡ç”¨æ—¶: {total_minutes:.0f}åˆ†{total_seconds:.1f}ç§’")
    
    print("=" * 80)


if __name__ == "__main__":
    print("=" * 80)
    print("QAèšç±»åˆ†æ - å¤šæ•°æ®é›†å¤„ç†æµç¨‹")
    print("=" * 80)
    
    # å¤„ç†åˆå¹¶çš„æ–°æ•°æ®é›† (tianmao + overseas)
    print("\n>>> é˜¶æ®µ1: æ•°æ®é›†åˆå¹¶å’Œç‰¹å¾æå–")
    stage1_merge_datasets()
    
    print("\n>>> é˜¶æ®µ2: åˆå¹¶æ•°æ®é›†èšç±»åˆ†æ")
    stage2_clustering_merged(choose_k=False, fixed_k=30)
    
    print("\n>>> é˜¶æ®µ3: åˆå¹¶æ•°æ®é›†ç­”æ¡ˆåˆ†ç»„å’Œæ ‡ç­¾ç”Ÿæˆ")
    stage3_answer_grouping_merged()
    
    print("\n" + "=" * 80)
    print("æ‰€æœ‰å¤„ç†å®Œæˆï¼")
    print("è¾“å‡ºæ–‡ä»¶:")
    print("  - merged_cluster_answers.xlsx: æœ€ç»ˆåˆå¹¶èšç±»ç»“æœï¼ˆåŒ…å«ä¸­æ–‡æ ‡ç­¾ï¼‰")
    print("  - merged_cluster.xlsx: åŸå§‹èšç±»åˆ†é…")
    print("  - merged_cluster_summary.xlsx: èšç±»æ‘˜è¦")
    print("=" * 80)
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print_final_statistics()
